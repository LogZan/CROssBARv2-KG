# STRING æ•°æ®å¤„ç†å¹¶è¡ŒåŒ–å¯è¡Œæ€§åˆ†æ

## ğŸ¯ å½“å‰çŠ¶å†µ

**ä¸²è¡Œå¤„ç†**:
```python
for tax in tqdm(self.tax_ids, desc="Retrieving STRING data"):  # 12,535 æ¬¡å¾ªç¯
    organism_string_ints = [
        i for i in string.string_links_interactions(
            ncbi_tax_id=int(tax),
            score_threshold="high_confidence",
        )
        if i.protein_a in self.string_to_uniprot
        and i.protein_b in self.string_to_uniprot
    ]
    self.string_ints.extend(organism_string_ints)
```

**æ—¶é—´**: 12,535 ç‰©ç§ Ã— 15ç§’ = **52.2 å°æ—¶**

## âœ… å¹¶è¡ŒåŒ–çš„å¯è¡Œæ€§

### ä¸ºä»€ä¹ˆé€‚åˆå¹¶è¡Œï¼Ÿ

1. **ä»»åŠ¡ç‹¬ç«‹æ€§** â­â­â­â­â­
   - æ¯ä¸ªç‰©ç§çš„å¤„ç†å®Œå…¨ç‹¬ç«‹
   - ä¸éœ€è¦ç‰©ç§ä¹‹é—´çš„æ•°æ®äº¤æ¢
   - æ²¡æœ‰é¡ºåºä¾èµ–å…³ç³»
   - **è¿™æ˜¯ç†æƒ³çš„å¹¶è¡Œåœºæ™¯ï¼**

2. **æ•°æ®å±€éƒ¨æ€§**
   - æ¯ä¸ªç‰©ç§è¯»å–è‡ªå·±çš„ç¼“å­˜æ–‡ä»¶
   - ç»“æœå¯ä»¥ç‹¬ç«‹æ”¶é›†ååˆå¹¶
   - ä¸å­˜åœ¨ç«äº‰æ¡ä»¶

3. **ä»»åŠ¡ç²’åº¦åˆé€‚**
   - æ¯ä¸ªä»»åŠ¡ ~15ç§’ï¼Œè¶³å¤Ÿå¤§
   - è¿›ç¨‹åˆ›å»ºå¼€é”€å¯ä»¥å¿½ç•¥
   - è´Ÿè½½å‡è¡¡å®¹æ˜“å®ç°

### ç†è®ºåŠ é€Ÿæ¯”

å‡è®¾æœ‰ N ä¸ª CPU æ ¸å¿ƒï¼š

```
ä¸²è¡Œæ—¶é—´:   52.2 å°æ—¶
å¹¶è¡Œæ—¶é—´:   52.2 / N å°æ—¶
åŠ é€Ÿæ¯”:     N Ã— (æ•ˆç‡å› å­)

ç¤ºä¾‹:
8æ ¸:   52.2 / 8  = 6.5 å°æ—¶   (åŠ é€Ÿ 8Ã—)
16æ ¸:  52.2 / 16 = 3.3 å°æ—¶   (åŠ é€Ÿ 15.8Ã—)
32æ ¸:  52.2 / 32 = 1.6 å°æ—¶   (åŠ é€Ÿ 30Ã—)
64æ ¸:  52.2 / 64 = 0.8 å°æ—¶   (åŠ é€Ÿ 58Ã—)
```

**æ•ˆç‡å› å­é€šå¸¸åœ¨ 0.9-0.95** (ç”±äºè¿›ç¨‹é€šä¿¡ã€è´Ÿè½½ä¸å‡ç­‰)

## âš ï¸ éœ€è¦è€ƒè™‘çš„æŒ‘æˆ˜

### 1. Python GIL (å…¨å±€è§£é‡Šå™¨é”)

**é—®é¢˜**: Python çš„çº¿ç¨‹å— GIL é™åˆ¶ï¼Œæ— æ³•çœŸæ­£å¹¶è¡Œ

**è§£å†³æ–¹æ¡ˆ**:
```python
# âŒ ä¸è¦ç”¨ threading (å— GIL é™åˆ¶)
import threading

# âœ… ä½¿ç”¨ multiprocessing (ç»•è¿‡ GIL)
import multiprocessing

# âœ… æˆ–ä½¿ç”¨ concurrent.futures
from concurrent.futures import ProcessPoolExecutor
```

**ä¸ºä»€ä¹ˆ multiprocessing æœ‰æ•ˆ**:
- åˆ›å»ºç‹¬ç«‹çš„ Python è¿›ç¨‹
- æ¯ä¸ªè¿›ç¨‹æœ‰è‡ªå·±çš„ GIL
- CPU å¯†é›†å‹ä»»åŠ¡å¯ä»¥çœŸæ­£å¹¶è¡Œ

### 2. å†…å­˜é™åˆ¶

**å½“å‰ä¸²è¡Œå¤„ç†**:
- åªæœ‰ 1 ä¸ªç‰©ç§åœ¨å†…å­˜ä¸­
- å³°å€¼å†…å­˜ â‰ˆ å•ä¸ªç‰©ç§çš„æ•°æ®é‡

**å¹¶è¡Œå¤„ç† (Nä¸ªè¿›ç¨‹)**:
- N ä¸ªç‰©ç§åŒæ—¶åœ¨å†…å­˜ä¸­
- å³°å€¼å†…å­˜ â‰ˆ N Ã— å•ä¸ªç‰©ç§æ•°æ®é‡

**ä¼°ç®—** (äººç±»æ˜¯æœ€å¤§çš„ç‰©ç§):
```
å•ä¸ªç‰©ç§å†…å­˜å ç”¨:
  - physical interactions: 1.4M Ã— 200 bytes â‰ˆ 280 MB
  - links: ç±»ä¼¼è§„æ¨¡
  - å­—å…¸å’Œå…¶ä»–: ~200 MB
  - æ€»è®¡: ~800 MB

8ä¸ªè¿›ç¨‹:  8 Ã— 800 MB = 6.4 GB     âœ“ é€šå¸¸å¯ä»¥
16ä¸ªè¿›ç¨‹: 16 Ã— 800 MB = 12.8 GB   âœ“ ä¸­ç­‰æœåŠ¡å™¨
32ä¸ªè¿›ç¨‹: 32 Ã— 800 MB = 25.6 GB   âš ï¸ éœ€è¦å¤§å†…å­˜
64ä¸ªè¿›ç¨‹: 64 Ã— 800 MB = 51.2 GB   âš ï¸ éœ€è¦é«˜ç«¯æœåŠ¡å™¨
```

**æ³¨æ„**: å¤§å¤šæ•°ç‰©ç§æ•°æ®é‡è¿œå°äºäººç±»ï¼Œå®é™…å†…å­˜å ç”¨ä¼šä½å¾ˆå¤š

### 3. I/O ç“¶é¢ˆ

**ç£ç›˜ I/O ç‰¹æ€§**:
- SSD éšæœºè¯»: ~50,000 IOPS
- ä¸²è¡Œè¯» vs å¹¶è¡Œè¯»å½±å“ä¸å¤§
- **å½“å‰ç“¶é¢ˆæ˜¯ CPU (è§£å‹+è§£æ)ï¼Œä¸æ˜¯ I/O**

**æµ‹è¯•æ•°æ®**:
- æ–‡ä»¶è¯»å–é€Ÿåº¦: 2.4M lines/s (å¾ˆå¿«)
- ä¸»è¦æ—¶é—´åœ¨è§£å‹å’Œè§£æ (CPU)

**ç»“è®º**: âœ“ I/O ä¸æ˜¯ç“¶é¢ˆï¼Œå¹¶è¡Œä¸ä¼šé€ æˆ I/O ç«äº‰

### 4. ç¼“å­˜æ–‡ä»¶æŸåé—®é¢˜

**å‘ç°çš„é—®é¢˜**:
```
zlib.error: Error -3 while decompressing data: invalid block type
IndexError: list index out of range
```

**éœ€è¦å¤„ç†**:
- é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- æŸåæ–‡ä»¶çš„è·³è¿‡æˆ–é‡æ–°ä¸‹è½½
- æ—¥å¿—è®°å½•å¤±è´¥çš„ç‰©ç§

## ğŸ’» å®ç°æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: ProcessPoolExecutor (æ¨è) â­â­â­â­â­

**ä¼˜ç‚¹**:
- ç®€å•æ˜“ç”¨
- è‡ªåŠ¨è´Ÿè½½å‡è¡¡
- å†…ç½®é”™è¯¯å¤„ç†
- èµ„æºç®¡ç†è‡ªåŠ¨

**ä»£ç ç¤ºä¾‹**:
```python
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm

def process_single_organism(tax_id, string_to_uniprot, cache=True):
    """å¤„ç†å•ä¸ªç‰©ç§çš„ STRING æ•°æ®"""
    try:
        organism_string_ints = [
            i for i in string.string_links_interactions(
                ncbi_tax_id=int(tax_id),
                score_threshold="high_confidence",
            )
            if i.protein_a in string_to_uniprot
            and i.protein_b in string_to_uniprot
        ]
        return tax_id, organism_string_ints, None
    except Exception as e:
        return tax_id, [], str(e)

def download_string_data_parallel(self, cache=True, n_workers=8):
    """å¹¶è¡Œä¸‹è½½ STRING æ•°æ®"""
    # å‡†å¤‡ string_to_uniprot æ˜ å°„ (æ‰€æœ‰è¿›ç¨‹å…±äº«)
    uniprot_to_string = uniprot.uniprot_data(
        fields="xref_string",
        organism=self.organism,
        reviewed=True
    )
    
    self.string_to_uniprot = collections.defaultdict(list)
    for k, v in uniprot_to_string.items():
        for string_id in list(filter(None, str(v).split(";"))):
            if "." in string_id:
                self.string_to_uniprot[string_id.split(".")[1]].append(k)
    
    # è¿‡æ»¤æœ‰é—®é¢˜çš„ tax IDs
    valid_tax_ids = [
        tax for tax in self.tax_ids 
        if tax not in tax_ids_to_be_skipped
    ]
    
    # å¹¶è¡Œå¤„ç†
    all_results = []
    failed_organisms = []
    
    with ProcessPoolExecutor(max_workers=n_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {
            executor.submit(
                process_single_organism, 
                tax_id, 
                dict(self.string_to_uniprot),  # è½¬ä¸ºæ™®é€šdictä¼ é€’
                cache
            ): tax_id 
            for tax_id in valid_tax_ids
        }
        
        # æ”¶é›†ç»“æœ
        for future in tqdm(as_completed(futures), 
                          total=len(futures),
                          desc="Retrieving STRING data (parallel)"):
            tax_id = futures[future]
            try:
                tax_id, organism_ints, error = future.result()
                if error:
                    logger.warning(f"Failed to process organism {tax_id}: {error}")
                    failed_organisms.append((tax_id, error))
                else:
                    all_results.extend(organism_ints)
            except Exception as e:
                logger.error(f"Error processing organism {tax_id}: {e}")
                failed_organisms.append((tax_id, str(e)))
    
    self.string_ints = all_results
    
    # æŠ¥å‘Šç»“æœ
    logger.info(f"Processed {len(valid_tax_ids) - len(failed_organisms)}/{len(valid_tax_ids)} organisms")
    if failed_organisms:
        logger.warning(f"Failed organisms: {len(failed_organisms)}")
        for tax_id, error in failed_organisms[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            logger.warning(f"  {tax_id}: {error}")
```

**ä¼˜åŒ–å»ºè®®**:
```python
# æ ¹æ®å¯ç”¨å†…å­˜è‡ªåŠ¨è°ƒæ•´å¹¶å‘æ•°
import psutil

def get_optimal_workers():
    """æ ¹æ® CPU å’Œå†…å­˜è‡ªåŠ¨ç¡®å®šæœ€ä¼˜è¿›ç¨‹æ•°"""
    cpu_count = multiprocessing.cpu_count()
    available_memory_gb = psutil.virtual_memory().available / (1024**3)
    
    # å‡è®¾æ¯ä¸ªè¿›ç¨‹éœ€è¦ 1GB å†…å­˜ (ä¿å®ˆä¼°è®¡)
    memory_workers = int(available_memory_gb / 1.0)
    
    # å– CPU å’Œå†…å­˜çš„è¾ƒå°å€¼ï¼Œä½†è‡³å°‘1ä¸ª
    optimal = min(cpu_count, memory_workers)
    return max(1, optimal - 2)  # ä¿ç•™2ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿ

n_workers = get_optimal_workers()
```

### æ–¹æ¡ˆ 2: multiprocessing.Pool

**ä»£ç ç¤ºä¾‹**:
```python
from multiprocessing import Pool

def download_string_data_parallel_pool(self, cache=True, n_workers=8):
    # ... å‡†å¤‡å·¥ä½œåŒä¸Š ...
    
    # ä½¿ç”¨ Pool
    with Pool(processes=n_workers) as pool:
        # ä½¿ç”¨ imap_unordered è·å¾—æ›´å¥½çš„è¿›åº¦æ˜¾ç¤º
        results = pool.imap_unordered(
            lambda tax: process_single_organism(
                tax, 
                dict(self.string_to_uniprot), 
                cache
            ),
            valid_tax_ids,
            chunksize=10  # æ¯æ¬¡åˆ†é…10ä¸ªä»»åŠ¡
        )
        
        # æ”¶é›†ç»“æœ
        for tax_id, organism_ints, error in tqdm(results, total=len(valid_tax_ids)):
            if not error:
                all_results.extend(organism_ints)
            else:
                failed_organisms.append((tax_id, error))
```

### æ–¹æ¡ˆ 3: åˆ†æ‰¹å¤„ç† (å†…å­˜å—é™æ—¶)

**å¦‚æœå†…å­˜ä¸è¶³ä»¥è¿è¡Œå¾ˆå¤šè¿›ç¨‹**:
```python
def download_string_data_batched(self, cache=True, batch_size=100, n_workers=8):
    """åˆ†æ‰¹å¹¶è¡Œå¤„ç†"""
    valid_tax_ids = [...]
    
    # åˆ†æˆæ‰¹æ¬¡
    for i in range(0, len(valid_tax_ids), batch_size):
        batch = valid_tax_ids[i:i+batch_size]
        
        # å¹¶è¡Œå¤„ç†è¿™ä¸ªæ‰¹æ¬¡
        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            futures = {...}
            # ... åŒä¸Š ...
        
        # å®šæœŸæ¸…ç†å†…å­˜
        gc.collect()
```

## ğŸ“Š é¢„æœŸæ€§èƒ½æå‡

### æµ‹è¯•ç¯å¢ƒå‡è®¾

| é…ç½® | CPUæ ¸å¿ƒ | å†…å­˜ | é¢„è®¡æ—¶é—´ | åŠ é€Ÿæ¯” |
|------|---------|------|----------|--------|
| å½“å‰ (ä¸²è¡Œ) | 1 | 2GB | 52.2å°æ—¶ | 1Ã— |
| å°å‹æœåŠ¡å™¨ | 8 | 16GB | 6.5å°æ—¶ | 8Ã— |
| ä¸­å‹æœåŠ¡å™¨ | 16 | 32GB | 3.3å°æ—¶ | 16Ã— |
| å¤§å‹æœåŠ¡å™¨ | 32 | 64GB | 1.6å°æ—¶ | 32Ã— |
| é«˜ç«¯æœåŠ¡å™¨ | 64 | 128GB | 0.8å°æ—¶ | 64Ã— |

### å®é™…æ•ˆç‡å› å­

è€ƒè™‘å®é™…å¼€é”€:
- è¿›ç¨‹å¯åŠ¨: ~1%
- æ•°æ®ä¼ é€’: ~2%
- è´Ÿè½½ä¸å‡: ~3%
- **æ€»æ•ˆç‡: ~94%**

**å®é™…åŠ é€Ÿæ¯”** â‰ˆ N Ã— 0.94

### ä¼˜åŒ–æ½œåŠ›

1. **åŠ¨æ€è´Ÿè½½å‡è¡¡**:
   - å°ç‰©ç§å¤„ç†å¿«
   - å¤§ç‰©ç§å¤„ç†æ…¢
   - ä½¿ç”¨ `imap_unordered` è‡ªåŠ¨å¹³è¡¡

2. **chunksize è°ƒä¼˜**:
   ```python
   # å° chunksize: æ›´å¥½çš„è´Ÿè½½å‡è¡¡ï¼Œä½†å¼€é”€å¤§
   # å¤§ chunksize: å¼€é”€å°ï¼Œä½†å¯èƒ½è´Ÿè½½ä¸å‡
   optimal_chunksize = max(1, len(tax_ids) // (n_workers * 4))
   ```

3. **é”™è¯¯æ¢å¤**:
   ```python
   # å¤±è´¥çš„ç‰©ç§å¯ä»¥é‡è¯•
   if failed_organisms:
       retry_with_fallback(failed_organisms)
   ```

## âš¡ å®æ–½æ­¥éª¤

### 1. å‡†å¤‡å·¥ä½œ
```bash
# æ£€æŸ¥ç³»ç»Ÿèµ„æº
python -c "import multiprocessing, psutil; \
    print(f'CPU: {multiprocessing.cpu_count()}'); \
    print(f'Memory: {psutil.virtual_memory().available / 1024**3:.1f} GB')"
```

### 2. æµ‹è¯•å°è§„æ¨¡
```python
# å…ˆç”¨ 10 ä¸ªç‰©ç§æµ‹è¯•
test_tax_ids = self.tax_ids[:10]
# è¿è¡Œå¹¶è¡Œç‰ˆæœ¬
# éªŒè¯ç»“æœæ­£ç¡®æ€§
```

### 3. é€æ­¥æ‰©å¤§
```python
# 100 ç‰©ç§
# 1000 ç‰©ç§  
# å…¨éƒ¨ 12535 ç‰©ç§
```

### 4. ç›‘æ§å’Œè°ƒä¼˜
```python
# ç›‘æ§å†…å­˜ä½¿ç”¨
# è°ƒæ•´ n_workers
# ä¼˜åŒ– chunksize
```

## ğŸ¯ æ¨èé…ç½®

### ä¿å®ˆé…ç½® (ç¨³å®š)
```python
n_workers = min(8, cpu_count() - 2)
chunksize = 10
```
- é€‚åˆ: å…±äº«æœåŠ¡å™¨ï¼Œå†…å­˜<32GB
- é¢„è®¡æ—¶é—´: ~6.5 å°æ—¶
- åŠ é€Ÿæ¯”: ~7-8Ã—

### å¹³è¡¡é…ç½® (æ¨è)
```python
n_workers = min(16, cpu_count() - 2)
chunksize = 5
```
- é€‚åˆ: ä¸“ç”¨æœåŠ¡å™¨ï¼Œå†…å­˜32-64GB
- é¢„è®¡æ—¶é—´: ~3.3 å°æ—¶
- åŠ é€Ÿæ¯”: ~15Ã—

### æ¿€è¿›é…ç½® (æœ€å¿«)
```python
n_workers = min(32, cpu_count() - 4)
chunksize = 3
```
- é€‚åˆ: é«˜æ€§èƒ½æœåŠ¡å™¨ï¼Œå†…å­˜>64GB
- é¢„è®¡æ—¶é—´: ~1.6 å°æ—¶
- åŠ é€Ÿæ¯”: ~30Ã—

## ğŸ’¡ å…¶ä»–ä¼˜åŒ–å»ºè®®

### 1. é¢„è¿‡æ»¤ string_to_uniprot

**å½“å‰**: æ¯ä¸ªè¿›ç¨‹é‡å¤ä¼ é€’æ•´ä¸ªå­—å…¸
**ä¼˜åŒ–**: åªä¼ é€’éœ€è¦çš„éƒ¨åˆ†

```python
# è¯»å–ç‰©ç§çš„è›‹ç™½è´¨åˆ—è¡¨
organism_proteins = get_organism_proteins(tax_id)
# åªä¼ é€’ç›¸å…³çš„æ˜ å°„
relevant_mapping = {
    k: v for k, v in string_to_uniprot.items()
    if any(p in organism_proteins for p in v)
}
```

### 2. ä½¿ç”¨å…±äº«å†…å­˜ (Python 3.8+)

```python
from multiprocessing import shared_memory

# å°† string_to_uniprot æ”¾å…¥å…±äº«å†…å­˜
# æ‰€æœ‰è¿›ç¨‹å…±äº«ï¼Œä¸éœ€è¦å¤åˆ¶
```

### 3. ç»“æœæµå¼å†™å…¥

```python
# ä¸è¦åœ¨å†…å­˜ä¸­ç´¯ç§¯æ‰€æœ‰ç»“æœ
# æ¯ä¸ªè¿›ç¨‹çš„ç»“æœç›´æ¥å†™å…¥æ–‡ä»¶
# æœ€ååˆå¹¶
```

## ğŸ“ æ€»ç»“

### âœ… å¹¶è¡ŒåŒ–éå¸¸å¯è¡Œ

**ä¼˜åŠ¿**:
- ä»»åŠ¡å®Œå…¨ç‹¬ç«‹
- CPU å¯†é›†å‹ (ä¸å— I/O é™åˆ¶)
- å¯ä»¥è·å¾—æ¥è¿‘çº¿æ€§çš„åŠ é€Ÿæ¯”

**é¢„æœŸæ”¶ç›Š**:
- 8æ ¸: 52å°æ—¶ â†’ 6.5å°æ—¶ (èŠ‚çœ 45.5å°æ—¶)
- 16æ ¸: 52å°æ—¶ â†’ 3.3å°æ—¶ (èŠ‚çœ 48.9å°æ—¶)
- 32æ ¸: 52å°æ—¶ â†’ 1.6å°æ—¶ (èŠ‚çœ 50.6å°æ—¶)

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **å†…å­˜ç®¡ç†**: æ ¹æ®å¯ç”¨å†…å­˜è°ƒæ•´å¹¶å‘æ•°
2. **é”™è¯¯å¤„ç†**: æŸäº›ç‰©ç§å¯èƒ½å¤±è´¥ï¼Œéœ€è¦ä¼˜é›…å¤„ç†
3. **èµ„æºç›‘æ§**: é¿å…è¿‡è½½ç³»ç»Ÿ

### ğŸ¯ æ¨èæ–¹æ¡ˆ

**ä½¿ç”¨ ProcessPoolExecutor + è‡ªåŠ¨è°ƒä¼˜**:
- ç®€å•å®ç°
- å¯é ç¨³å®š
- æ˜“äºè°ƒè¯•
- è‡ªåŠ¨è´Ÿè½½å‡è¡¡

**æŠ•å…¥äº§å‡ºæ¯”**: â­â­â­â­â­
- å¼€å‘æ—¶é—´: 2-4 å°æ—¶
- æµ‹è¯•æ—¶é—´: 2-4 å°æ—¶
- èŠ‚çœæ—¶é—´: 45+ å°æ—¶ (æ¯æ¬¡è¿è¡Œ)

---

**ç»“è®º**: å¹¶è¡ŒåŒ–æ˜¯éå¸¸å€¼å¾—çš„ä¼˜åŒ–ï¼Œå¯ä»¥å°†å¤„ç†æ—¶é—´ä» 52 å°æ—¶é™ä½åˆ° 1.6-6.5 å°æ—¶ï¼
